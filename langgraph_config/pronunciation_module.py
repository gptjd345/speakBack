from .store import global_store
import re
from typing import Optional, Dict

from vosk import Model, KaldiRecognizer
import wave, json, os

from pydub import AudioSegment
import io
import subprocess

# -----------------------------
# Audio ì „ì²˜ë¦¬ (BytesIO â†’ 16kHz mono wav)
# -----------------------------
def prepare_audio_for_vosk(org_file_path) -> io.BytesIO:
    """
    Streamlit BytesIO / UploadedFile â†’ Voskì—ì„œ ì“¸ ìˆ˜ ìˆëŠ” 16kHz mono wavë¡œ ë³€í™˜
    """
    process = subprocess.run(
        [
            "ffmpeg",
            "-i", org_file_path,
            "-ar", "16000",    # 16kHz
            "-ac", "1",        # mono
            "-f", "wav",
            "pipe:1"
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        check=True
    )
    return io.BytesIO(process.stdout)

# -----------------------------
# Vosk ëª¨ë¸ ë¡œë“œ
# -----------------------------
VOSK_MODEL_PATH = "vosk-model-small-en-us-0.15"  # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ ê²½ë¡œ
if not os.path.exists(VOSK_MODEL_PATH):
    raise FileNotFoundError("Vosk ëª¨ë¸ì„ ë¨¼ì € ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”!")
vosk_model = Model(VOSK_MODEL_PATH)

def stt_vosk(user_audio_path: str) :
    """ì‚¬ìš©ì ìŒì„±íŒŒì¼ì„ Vosk STTë¡œ ë³€í™˜"""
    audio_stream = prepare_audio_for_vosk(user_audio_path)

    # Vosk recognizer ì´ˆê¸°í™”
    model = Model(VOSK_MODEL_PATH)
    rec = KaldiRecognizer(model, 16000)
    rec.SetWords(True)

    # wav header skip í•„ìš” â†’ wave ëª¨ë“ˆ ì‚¬ìš© X, raw bytes ê·¸ëŒ€ë¡œ ì²˜ë¦¬
    while True:
        data = audio_stream.read(4000)
        if len(data) == 0:
            break
        rec.AcceptWaveform(data)

    result = json.loads(rec.FinalResult())
    text = result.get("text", "").strip()
    words = result.get("result", [])  # ë‹¨ì–´ë³„ confidence

    conf_dict = {w["word"]: w.get("conf", 0) for w in words}
    return text, conf_dict

# Try import Coqui TTS (optional)
try:
    from TTS.api import TTS
    TTS_AVAILABLE = True
except Exception:
    TTS_AVAILABLE = False

# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ
# ëª¨ë¸ ì„ íƒ: (ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì´ë¦„ì€ í™˜ê²½ì— ë”°ë¼ ë°”ê¿”ì•¼ í•¨)
# ì˜ˆ: LJSpeech â†’ ë¯¸êµ­ ì—¬ì„± í™”ì ë°ì´í„°ì…‹ ê¸°ë°˜
# ë°œìŒì€ ì „í˜•ì ì¸ American English
tts_us_model = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False, gpu=False)
#tts_uk_model = TTS(model_name="tts_models/en/vctk/vits", progress_bar=False, gpu=False)

# ë‹¨ìˆœ function words ë¦¬ìŠ¤íŠ¸ (ë¹ ì ¸ë„ ë˜ëŠ”ê²½ìš°ê°€ ë§ì€ ë‹¨ì–´ë“¤)
# ê¸°ëŠ¥ì–´ ë¦¬ìŠ¤íŠ¸
FUNCTION_WORDS = set([
    "a", "an", "the",
    "is", "am", "are", "was", "were", "be", "been", "being",
    "do", "does", "did",
    "have", "has", "had",
    "will", "would", "shall", "should", "can", "could", "may", "might", "must",
    "to", "of", "in", "on", "at", "for", "with", "from", "by",
    "and", "but", "or", "so", "because",
    "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them"
])

# ì¶•ì•½(ê¶Œì¥) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸: í‚¤ëŠ” canonical, ê°’ì€ í—ˆìš©ë˜ëŠ” ì¶•ì•½ íŒ¨í„´ë“¤(ì •ê·œí‘œí˜„ì‹)
CONTRACTION_WHITELIST = {
    "could have": [r"coulda", r"could've", r"could of"],
    "would have": [r"woulda", r"would've", r"would of"],
    "should have": [r"shoulda", r"should've", r"should of"],
    "going to": [r"gonna", r"gon?na"],
    "want to": [r"wanna"],
    "want a": [r"gimme", r"lemme", r"gonna"],  # ì˜ˆì‹œ
    "let me": [r"lemme"],
    "give me": [r"gimme"],
    "I am": [r"I'm", r"Im", r"i'm"],
    "do not": [r"don't", r"dont"],
}

def score_content_word(w, user_tokens, conf_dict):
    """ë‚´ìš©ì–´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜â†‘, confidence ê¸°ì¤€â†‘)"""
    conf = conf_dict.get(w, 0)
    if w in user_tokens and conf >= 0.6:
        return 2.0
    elif conf >= 0.55:
        return 1.8
    elif conf >= 0.4:
        return 1.6
    else:
        return 0.0

def score_function_word(w, user_tokens, conf_dict):
    """ê¸°ëŠ¥ì–´ ì ìˆ˜ ê³„ì‚° (ë³´ë„ˆìŠ¤, confidence ê¸°ì¤€â†‘)"""
    conf = conf_dict.get(w, 0)
    gained = 0.0
    if w in user_tokens:
        # ì¶•ì•½ í™•ì¸
        for base, patterns in CONTRACTION_WHITELIST.items():
            if w in base.split():
                if any(re.fullmatch(pat, ut) for ut in user_tokens for pat in patterns):
                    if conf >= 0.6:
                        return 2.5
        if conf >= 0.6:
            gained = 1.5
        elif conf >= 0.5:
            gained = 1.2
    elif conf >= 0.4:
        gained = 0.8
    return gained

# -----------------------------
# ê°„ë‹¨í•œ ì²­í‚¹ í•¨ìˆ˜
# -----------------------------
def chunk_sentence(text: str):
    """
    ì˜ì–´ ë¬¸ì¥ì„ ì›ì–´ë¯¼ ë¦¬ë“¬ì— ë§ì¶° ëŒ€ëµì ì¸ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    """
    patterns = [r"\b(and|but|or|so|because)\b",
                r"\b(could have|would have|should have|going to|want to|let me|give me)\b",
                r"\b(in|on|at|for|with|from|by|to|of)\b"]

    text = text.lower()
    for p in patterns:
        text = re.sub(p, r"@@\1@@", text)

    raw_chunks = [c.strip() for c in text.split("@@") if c.strip()]
    chunks = [re.findall(r"[a-zA-Z']+", c) for c in raw_chunks]
    return chunks

  
def check_contraction(user_transcript: str, target_phrase: str) -> bool:
    """ì‚¬ìš©ìê°€ í—ˆìš©ëœ ì¶•ì•½í˜•ì„ ì¼ëŠ”ì§€ í™•ì¸"""
    if target_phrase in CONTRACTION_WHITELIST:
        for pattern in CONTRACTION_WHITELIST[target_phrase]:
            if re.search(pattern, user_transcript, re.IGNORECASE):
                return True
    return False

# -----------------------------
# TTS ìƒì„± (ê¸¸ì´ í¬í•¨)
# -----------------------------
def tts_generate_us(text: str) -> bytes:
    """US tutor TTS â†’ wav ë°”ì´íŠ¸ ë¦¬í„´"""
    wav_path = "reference_us.wav"
    tts_us_model.tts_to_file(text=text, file_path=wav_path)
    with open(wav_path, "rb") as f:
        wav_bytes = f.read()

    seg = AudioSegment.from_file(io.BytesIO(wav_bytes), format="wav")
    duration_sec = len(seg) / 1000.0

    return wav_bytes, duration_sec
    
# -----------------------------
# Audio duration helper
# -----------------------------
def get_audio_duration(file_path: str) -> float:
    """wav íŒŒì¼ ê¸¸ì´(ì´ˆ)"""
    with wave.open(file_path, "rb") as wf:
        frames = wf.getnframes()
        rate = wf.getframerate()
        return frames / float(rate)   

def evaluate_pronunciation(target_text: str, user_audio_path: str, tutor_type: str = "us"):
    """
    ì „ì²´ íë¦„: ì‚¬ìš©ì ì˜¤ë””ì˜¤ â†’ STT â†’ ë°œìŒ í‰ê°€ â†’ ê²°ê³¼ ë°˜í™˜
    """
    # 1) íŠœí„° ì°¸ì¡° ìŒì„±, íŠœí„° ìŒì„±ì‹œê°„
    ref_audio,ref_duration = tts_generate_us(target_text) if tutor_type == "us" else None

    # 2) ì‚¬ìš©ì ìŒì„± â†’ STT
    user_transcript, conf_dict = stt_vosk(user_audio_path)
    user_tokens = re.findall(r"[a-zA-Z']+", user_transcript.lower())
    user_seg = AudioSegment.from_file(user_audio_path)
    # ì‚¬ìš©ìë°œí™”ì‹œê°„
    user_duration = len(user_seg) / 1000.0

    # 3) ì²­í¬í™”
    target_chunks = chunk_sentence(target_text)
    

    feedback = []
    score = 0
    total = 0

    # 4) ì²­í¬ ë¹„êµ
    for chunk in target_chunks:
        total += 2
        content_words = [w for w in chunk if w not in FUNCTION_WORDS and len(w) > 1]
        function_words = list(dict.fromkeys([w for w in chunk if w in FUNCTION_WORDS]))  # ì¤‘ë³µ ì œê±°

        # ë‚´ìš©ì–´ í‰ê°€
        for w in content_words:
            total += 2.0  # ê¸°ì¤€ì ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ
            gained = score_content_word(w, user_tokens, conf_dict)
            score += gained
            if gained == 2.0:
                feedback.append(f"ë‚´ìš©ì–´ '{w}'ëŠ” ë¶„ëª…íˆ ì˜ ë“¤ë ¸ì–´ìš” ğŸ‘")
            elif gained >= 1.5:
                feedback.append(f"ë‚´ìš©ì–´ '{w}'ëŠ” ëŒ€ì²´ë¡œ ì¢‹ì•˜ì§€ë§Œ ì¡°ê¸ˆ ë” ë˜ë ·í•˜ë©´ ì™„ë²½í•´ìš”.")
            elif gained >= 1.0:
                feedback.append(f"ë‚´ìš©ì–´ '{w}'ëŠ” ë“¤ë¦¬ê¸´ í–ˆì§€ë§Œ ì•½í–ˆì–´ìš”.")
            else:
                feedback.append(f"ë‚´ìš©ì–´ '{w}' ë°œìŒì„ ë†“ì¹œ ê²ƒ ê°™ì•„ìš”.")

        # ê¸°ëŠ¥ì–´ í‰ê°€
        for w in function_words:
            gained = score_function_word(w, user_tokens, conf_dict)
            score += gained
            if gained >= 0.8:
                feedback.append(f"'{w}'ë¥¼ ì¶•ì•½í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë§í–ˆë„¤ìš” ğŸ‘Œ")
            elif gained >= 0.5:
                feedback.append(f"ê¸°ëŠ¥ì–´ '{w}'ëŠ” ë¬´ë‚œíˆ ë°œìŒí–ˆì–´ìš”.")
            elif gained > 0:
                feedback.append(f"ê¸°ëŠ¥ì–´ '{w}'ëŠ” ì¡°ê¸ˆ ì•½í–ˆì–´ìš”.")
            else:
                feedback.append(f"ê¸°ëŠ¥ì–´ '{w}' ë°œìŒì´ ê±°ì˜ ì•ˆ ë“¤ë ¸ì–´ìš”.")
    
    # 5) ì†ë„ ë³´ë„ˆìŠ¤ (20%)
    if user_duration <= ref_duration + 5:
        bonus = (score / total) * 0.2
        score += bonus
        feedback.append("â±ï¸ ë°œí™” ì†ë„ê°€ ìì—°ìŠ¤ëŸ¬ì›Œì„œ ì¶”ê°€ ì ìˆ˜ë¥¼ ë“œë¦½ë‹ˆë‹¤!")

    print("DEBUG total",total)
    print("DEBUG score",score)
    percentage = round((score / total) * 100, 1)

    # ìµœì¢… ê²°ê³¼
    result = {
        "score": percentage,
        "feedback": feedback,
        "target_chunks": target_chunks,
        "reference_tts": ref_audio,   # US tutor ìŒì„± (wav ë°”ì´íŠ¸)
        "user_transcript": user_transcript,
        "user_duration": user_duration,
        "ref_duration": ref_duration
    }
    return result

    # store into global_store as well (synchronized)
    global_store.tts_us_comment = final_comment
    global_store.tts_us_audio = tts_bytes

"""
    out = {
        "comment": final_comment,
        "highlights": highlights,
        "tts_audio": tts_bytes
    }
    return out
"""
